{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import importlib\n",
    "import numpy as np\n",
    "import re\n",
    "import base64\n",
    "import ast\n",
    "import warnings\n",
    "import random\n",
    "import string\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr,pearsonr, kendalltau\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from scipy.stats import entropy, ttest_ind,ttest_rel\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import scripts.utils as utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  directory & file hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../..')\n",
    "stimuli_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "analysis_dir = os.path.join(proj_dir,'analysis')\n",
    "data_dir = os.path.join(proj_dir,'data')\n",
    "# plot_dir = os.path.join(results_dir,'plots')\n",
    "# csv_dir = os.path.join(results_dir,'csv')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'utils'))\n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SDI(data):\n",
    "#     \"\"\"\n",
    "#     Calculate the Simpson's Diversity Index for a list of data.\n",
    "#     \"\"\"\n",
    "#     N = np.sum(data)\n",
    "#     n = np.array(data)\n",
    "    \n",
    "#     p = n / N\n",
    "#     print(p)\n",
    "#     return 1 - np.sum(p ** 2)\n",
    "\n",
    "\n",
    "# def mm_normalize(row, min_max_dict):\n",
    "#     min_recog = min_max_dict[row['uniqueID']]['min']\n",
    "#     max_recog = min_max_dict[row['uniqueID']]['max']\n",
    "#     if (max_recog - min_recog) == 0:\n",
    "#         return row['mean_accuracy']\n",
    "#     norm_acc = (row['mean_accuracy'] - min_recog) / (max_recog - min_recog)\n",
    "#     return norm_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def compute_similarities(df, spose_embeds, spose_cols, random=False):\n",
    "#     similarities = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         if random:\n",
    "#             target_concept = np.random.choice(spose_embeds['concept'].unique())\n",
    "#         else:\n",
    "#             target_concept = row.uniqueID\n",
    "        \n",
    "#         target_embed = spose_embeds[spose_embeds['concept'] == target_concept][spose_cols].values[0]\n",
    "#         responses = row.response_list\n",
    "#         response_embeds = [spose_embeds[spose_embeds['concept'] == resp][spose_cols].values[0] for resp in responses]\n",
    "#         mean_response_embed = np.mean(response_embeds, axis=0)\n",
    "#         similarity_score = cosine_similarity(target_embed.reshape(1, -1), mean_response_embed.reshape(1, -1))[0][0]\n",
    "#         similarities.append(similarity_score)\n",
    "#     return similarities\n",
    "\n",
    "# def spose_permutation_test(df, spose_embeds, spose_cols, n_permutations=10):\n",
    "#     # Compute original similarities\n",
    "#     df['similarity_score'] = compute_similarities(df, spose_embeds, spose_cols)\n",
    "#     df_agg = df.groupby('uniqueID').agg({'similarity_score': 'mean', 'correct': 'mean'}).reset_index()\n",
    "#     original_r, _ = pearsonr(df_agg.similarity_score, df_agg.correct)\n",
    "    \n",
    "#     # Permutation test\n",
    "#     permuted_rs = []\n",
    "#     for _ in range(n_permutations):\n",
    "#         df['similarity_score'] = compute_similarities(df, spose_embeds, spose_cols, random=True)\n",
    "#         df_agg = df.groupby('uniqueID').agg({'similarity_score': 'mean', 'correct': 'mean'}).reset_index()\n",
    "#         r, _ = pearsonr(df_agg.similarity_score, df_agg.correct)\n",
    "#         permuted_rs.append(r)\n",
    "    \n",
    "#     p_value = np.mean(np.abs(permuted_rs) >= np.abs(original_r))\n",
    "    \n",
    "#     return original_r, permuted_rs, p_value\n",
    "\n",
    "# # Run the permutation test\n",
    "# r, permuted_rs, p_value = spose_permutation_test(things_draw_recog_df, spose_embeds, spose_cols)\n",
    "\n",
    "# print(f\"Original r: {r}\")\n",
    "# print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(permuted_rs, bins=50, edgecolor='black')\n",
    "# plt.axvline(r, color='red', linestyle='dashed', linewidth=2)\n",
    "# plt.title('Distribution of Permuted Correlation Coefficients')\n",
    "# plt.xlabel('Pearson r')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_worker_id(worker_id, salt='dot2025'):\n",
    "    \"\"\"\n",
    "    Encrypt a single worker ID using SHA-256 hash function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    worker_id : str or any\n",
    "        The worker ID to encrypt\n",
    "    salt : str, optional\n",
    "        A random string to add to the worker ID before hashing for additional security\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Encrypted worker ID as a hexadecimal string\n",
    "    \"\"\"\n",
    "    # Convert to string if not already\n",
    "    worker_id_str = str(worker_id)\n",
    "    # Add salt to the worker ID and encode to bytes\n",
    "    salted_id = (worker_id_str + salt).encode('utf-8')\n",
    "    # Create hash and return hex digest\n",
    "    return hashlib.sha256(salted_id).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_main_df = pd.read_csv(os.path.join(data_dir, 'things_concepts.tsv'), sep='\\t') ### things concepts metadata\n",
    "concept2uid = dict(zip(things_main_df['Word'], things_main_df['uniqueID'])) ### dataframe that maps  words to uniqueIDs\n",
    "things1854concepts = things_main_df.uniqueID.values ### list of things concepts\n",
    "concept2category_dict = dict(zip(things_main_df.uniqueID, things_main_df['All Bottom-up Categories'])) ### map the concepts to categories using wordnet(?) categories\n",
    "\n",
    "things_gpt_word_embeds = pd.read_csv(os.path.join(data_dir, 'things1854_gpt_embeddings.csv'), index_col=0) ### embeddings from GPT circa 2023\n",
    "things_gpt_def_embeds = pd.read_csv(os.path.join(data_dir, 'things1854_def_gpt_embeddings.csv'),index_col=0) ### embeddings from definitions of THINGS concepts 2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### list all the files in data_dir/things_draw_recog if they are csv files\n",
    "things_draw_recog_dir = os.path.join(data_dir,'thingsdraw_recog')\n",
    "things_draw_prod_dir = os.path.join(data_dir,'things_drawing_1854_complete')\n",
    "things_draw_recog_files = [f for f in os.listdir(things_draw_recog_dir) if os.path.isfile(os.path.join(things_draw_recog_dir, f)) and f.endswith('sketches.csv')]\n",
    "things_draw_prod_files = [f for f in os.listdir(things_draw_prod_dir) if os.path.isfile(os.path.join(things_draw_prod_dir, f)) and f.endswith('.csv')]\n",
    "### read in all the files in things_draw_recog_files and concatenate into a single dataframe\n",
    "things_draw_recog_df = pd.concat([pd.read_csv(os.path.join(things_draw_recog_dir,f),index_col='Unnamed: 0') for f in things_draw_recog_files],ignore_index=True)\n",
    "things_draw_prod_df = pd.concat([pd.read_csv(os.path.join(things_draw_prod_dir,f),index_col='Unnamed: 0') for f in things_draw_prod_files],ignore_index=True)\n",
    "things_draw_recog_demo_df = pd.read_csv(os.path.join(things_draw_recog_dir,'things_draw_recog_demographics.csv')) ### demographics for the recognition task\n",
    "\n",
    "qc_df = pd.read_csv(os.path.join(data_dir,'qc_df_tmp.csv')) #### quality control df \n",
    "valid_counts = qc_df.groupby('sketch_id')['valid'].sum() ### get the number of valid counts\n",
    "valid_ids = valid_counts[valid_counts>=2].index.values ### only include the sketches that have >=2 valid counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THINGS External data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THNGS Sprase Embeddings\n",
    "spose_embeds = pd.read_csv(os.path.join(data_dir,'THINGS_spose.txt'), sep='\\t', header=None) \n",
    "spose_cols = spose_embeds.columns.tolist()\n",
    "spose_embeds['concept']=things1854concepts\n",
    "\n",
    "#### THINGS Memorability data\n",
    "THINGS_mem = pd.read_csv(os.path.join(data_dir,'THINGS_memorability.csv'))\n",
    "\n",
    "### THINGS+ data\n",
    "things_plus_df= pd.read_csv(os.path.join(data_dir,'THINGSplus_categories.tsv'),sep='\\t')\n",
    "things_plus_dict = dict(zip(things_plus_df.uniqueID, things_plus_df.category))\n",
    "\n",
    "### THINGS image recognizability\n",
    "things_image_recongizability = pd.read_csv(os.path.join(data_dir,'THINGS_recognizability.csv'),index_col=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### look at which concepts are recognized the best on average (for images): \n",
    "things_image_recongizability.groupby('uniqueID')['recognizability'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute accuracy on practice trials\n",
    "recog_practice_trials = things_draw_recog_df[things_draw_recog_df['filename'].astype(str).isin(['cat','nan'])].reset_index(drop=True) \n",
    "recog_practice_trials['response_list'] = recog_practice_trials['response'].apply(lambda x: list(ast.literal_eval(x).values()))\n",
    "recog_practice_trials['accuracy'] = recog_practice_trials.apply(lambda x: any(animal in x['response_list'] for animal in ['cat', 'dog', 'kitten', 'fox','chihuaha','puppy']), axis=1)\n",
    "practice_trial_fail_ids = recog_practice_trials[recog_practice_trials.accuracy==False].workerID.to_list()\n",
    "\n",
    "print(f'mean accuracy on the cat drawing recognition trials: {recog_practice_trials[\"accuracy\"].mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects in drawing study prior to excluding for valid workerIDS: 1315\n",
      "Number of subjects in recognition study prior to excluding for valid workerIDS: 1557\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of subjects in drawing study prior to excluding for valid workerIDS: {len(things_draw_prod_df.workerID.unique())}\")\n",
    "print(f\"Number of subjects in recognition study prior to excluding for valid workerIDS: {len(things_draw_recog_df.workerID.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_exclusions = ['61bb389740db417c1a138cad'] ### exclude these workers from the analysis\n",
    "missing_meta_sketches = ['641b5bfc88da294e33e7779c', '641b5bae88da294e33e77519',\n",
    "       '641b5dac88da294e33e78290', '642b11d7d30b092e53f1ca40',\n",
    "       '642b2d962fc03e2456b46930', '642b55e7fb5d0a582eae4db6']  ### exclude these workers from the analysis\n",
    "\n",
    "things_draw_recog_df = things_draw_recog_df[~things_draw_recog_df['filename'].astype(str).isin(['cat','nan'])].reset_index(drop=True) ### remove practice trials\n",
    "things_draw_recog_df['response_list'] =  things_draw_recog_df['response'].apply(lambda x: list(ast.literal_eval(x).values()))\n",
    "\n",
    "### add category info \n",
    "things_draw_recog_df['category'] = things_draw_recog_df['uniqueID'].apply(lambda x: things_plus_dict[x] if x in things_plus_dict.keys() else 'other')\n",
    "things_draw_recog_df['cat_response_list'] = things_draw_recog_df['response_list'].apply(lambda x: [things_plus_dict[i] if i in things_plus_dict.keys() else 'other' for i in x])\n",
    "\n",
    "## remove any rows where response_list is empty\n",
    "things_draw_recog_df = things_draw_recog_df[things_draw_recog_df['response_list'].apply(lambda x: len(x)>0)].reset_index(drop=True)\n",
    "\n",
    "things_draw_recog_df['sketch_id']=things_draw_recog_df.filename_recog.apply(lambda x: str(x).split('.')[0])\n",
    "things_draw_recog_df['correct'] = things_draw_recog_df.apply(lambda x: x['uniqueID'] in (x['response_list']), axis=1)\n",
    "things_draw_recog_df['cat_correct'] = things_draw_recog_df.apply(lambda x: x['category'] in (x['cat_response_list']), axis=1)\n",
    "things_draw_recog_df['top1_correct'] = things_draw_recog_df.apply(lambda x: x['response_list'][0] == x['uniqueID'], axis=1)\n",
    "things_draw_recog_df['top1_cat_correct'] = things_draw_recog_df.apply(lambda x: x['cat_response_list'][0] == x['category'], axis=1)\n",
    "\n",
    "\n",
    "### do some exclusions\n",
    "things_draw_recog_df = things_draw_recog_df[things_draw_recog_df.sketch_id.isin(valid_ids)]\n",
    "things_draw_recog_df = things_draw_recog_df[~things_draw_recog_df['workerID'].isin(practice_trial_fail_ids)].reset_index(drop=True)\n",
    "things_draw_prod_df = things_draw_prod_df[~things_draw_prod_df['workerID'].isin(prod_exclusions)].reset_index(drop=True)\n",
    "things_draw_prod_df = things_draw_prod_df[~things_draw_prod_df['_id'].isin(missing_meta_sketches)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "sketch_trials_df = things_draw_prod_df[things_draw_prod_df.trial_type=='sketchpad'].sort_values(by=['sessionID','trial_index']).reset_index(drop=True)\n",
    "sketch_trials_df['display_label'] = sketch_trials_df['prompt'].apply(lambda x: x.split('>')[1].split('<')[0])   \n",
    "\n",
    "\n",
    "## familiarity trials\n",
    "fam_trials_df = things_draw_prod_df[(things_draw_prod_df.trial_type=='survey-multi-choice')&(things_draw_prod_df.trial_index<50)].sort_values(by=['sessionID','trial_index']).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sketches prior to excluding for valid workerIDS: {len(sketch_trials_df)}\")\n",
    "print(f\"Number of subjects prior to excluding for valid workerIDS: {len(things_draw_prod_df.workerID.unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_trials_df= sketch_trials_df[sketch_trials_df['workerID'].astype(str)!='nan']\n",
    "print(f\"Number of sketches after excluding for valid workerIDS: {len(sketch_trials_df)}\")\n",
    "sketch_trials_df = sketch_trials_df[sketch_trials_df['_id'].isin(valid_ids)].reset_index(drop=True)\n",
    "print(f\"Number of sketches after excluding for quality control sketches: {len(sketch_trials_df)}\")\n",
    "\n",
    "fam_trials_df= fam_trials_df[fam_trials_df['workerID'].astype(str)!='nan']\n",
    "\n",
    "## subset recognition data to only include sketches that we know to be valid\n",
    "things_draw_recog_df = things_draw_recog_df[things_draw_recog_df['sketch_id'].isin(sketch_trials_df['_id'].unique())].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_trials_df['response'] = fam_trials_df.apply(lambda x: ast.literal_eval(x.response),axis=1)\n",
    "fam_trials_df['recog'] = fam_trials_df.response.apply(lambda x: True if x['Q0']=='Yes' else False)\n",
    "mean_fam_df = fam_trials_df.groupby('concept')['recog'].mean().reset_index()\n",
    "\n",
    "for i,row in sketch_trials_df.iterrows():\n",
    "    this_fam_row = fam_trials_df[(fam_trials_df['sessionID']==row['sessionID']) \\\n",
    "                              & (fam_trials_df['trial_index']==row['trial_index']+1) &\\\n",
    "                              (fam_trials_df['concept']==row['concept'])]\n",
    "    if this_fam_row.shape[0]==0:\n",
    "        this_fam_row.append(row)\n",
    "        print('not matched')\n",
    "    else:\n",
    "        sketch_trials_df.loc[i,'familiar'] = this_fam_row.recog.values[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving out CSVs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rename _id to sketch_id\n",
    "sketch_trials_df['subject_id'] = sketch_trials_df['workerID'].apply(lambda x: encrypt_worker_id(x))\n",
    "sketch_trials_out = sketch_trials_df[['subject_id','_id','trial_index','concept','familiar','strokes','undo_history',\\\n",
    "                                      'rt','time_elapsed','pointer_device']]\n",
    "\n",
    "sketch_trials_out = sketch_trials_out.rename(columns={'_id':'sketch_id'})\n",
    "sketch_trials_out.to_csv(os.path.join(data_dir,'things-drawings-prod-clean.csv'),index=False)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_draw_recog_df['subject_id'] = things_draw_recog_df['workerID'].apply(lambda x: encrypt_worker_id(x))\n",
    "recog_trials_out = things_draw_recog_df[['subject_id','trial_index','rt','concept','uniqueID','response','filename',\n",
    "                                      'sketch_id','response_list']]\n",
    "recog_trials_out.to_csv(os.path.join(data_dir,'things-drawings-recog-clean.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things_draw_recog_df['familiar'] = things_draw_recog_df.apply(lambda x: sketch_trials_df[sketch_trials_df._id==x['sketch_id']]['familiar'].values[0],axis=1)\n",
    "\n",
    "## here for each sketch_id in the recognition task, we want to get the familiarity rating from the familiarity trials in the production task\n",
    "invalids=[]\n",
    "for i,row in things_draw_recog_df.iterrows():\n",
    "    this_sketch_row = sketch_trials_df[sketch_trials_df._id==row['sketch_id']]\n",
    "    if this_sketch_row.shape[0]==0:\n",
    "        invalids.append(row['sketch_id'])\n",
    "        print('not matched')\n",
    "    else:\n",
    "        things_draw_recog_df.loc[i,'familiar'] = this_sketch_row.familiar.values[0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this creates a drawing x concept recognition vector where the number in each cell tells us the number of times that drawing was labeled as that concept\n",
    "###this also takes a while to run\n",
    "recog_response_vec_df = things_draw_recog_df.explode('response_list').groupby(['filename','sketch_id',\\\n",
    "                                                    'concept','uniqueID','category','num_strokes','familiar'])['response_list'].value_counts().unstack(fill_value=0).reset_index()\n",
    "\n",
    "non_label_concepts = np.setdiff1d( things1854concepts, recog_response_vec_df.columns) ### labels that were never used\n",
    "## add all the elemnts of non_label_concepts to human_response_vec_df and set them to 0\n",
    "for concept in non_label_concepts:\n",
    "    recog_response_vec_df[concept]=0\n",
    "\n",
    "recog_response_vec_df['mean_accuracy']=recog_response_vec_df.apply(lambda x:things_draw_recog_df[things_draw_recog_df['filename']==x.filename].correct.mean(), axis=1)\n",
    "recog_response_vec_df['mean_top1_accuracy']=recog_response_vec_df.apply(lambda x:things_draw_recog_df[things_draw_recog_df['filename']==x.filename].top1_correct.mean(), axis=1)\n",
    "\n",
    "\n",
    "### this is the same as above but uses category labels from things plus instead of things object concepts\n",
    "recog_response_vec_cat_df = things_draw_recog_df.explode('cat_response_list').groupby(['filename','sketch_id',\\\n",
    "                                                    'concept','uniqueID','category','num_strokes','familiar'])['cat_response_list'].value_counts().unstack(fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "recog_response_vec_cat_df['mean_accuracy']=recog_response_vec_cat_df.apply(lambda x:things_draw_recog_df[things_draw_recog_df['filename']==x.filename].cat_correct.mean(), axis=1)\n",
    "recog_response_vec_cat_df['mean_top1_accuracy']=recog_response_vec_cat_df.apply(lambda x:things_draw_recog_df[things_draw_recog_df['filename']==x.filename].top1_cat_correct.mean(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sot items by things concept name\n",
    "recog_response_vec_df = recog_response_vec_df.sort_values(by=['uniqueID'])\n",
    "recog_response_vec_cat_df = recog_response_vec_cat_df.sort_values(by=['uniqueID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the number of labels for each sketch\n",
    "things_draw_recog_df['num_labels'] = things_draw_recog_df['response_list'].apply(lambda x: np.unique(np.array(x)).shape[0])\n",
    "print(f\"average number of sketches per concept: {things_draw_recog_df.groupby('concept').sketch_id.nunique().mean()}\")\n",
    "print(f\"max number of sketches per concept: {things_draw_recog_df.groupby('concept').sketch_id.nunique().max()}\")\n",
    "print(f\"min number of sketches per concept: {things_draw_recog_df.groupby('concept').sketch_id.nunique().min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### group things_draw_recog_df by uniqueID and count the number of unique '_id's within each group\n",
    "print(f\"median number of times each sketch was labeled: {things_draw_recog_df.groupby('filename')['_id'].count().median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average number of times each sketch was labeled:',things_draw_recog_df.groupby('sketch_id').size().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mean top k recognition accuracy is {things_draw_recog_df.correct.mean()},\\n\\\n",
    "and mean top k accuracy at the category level is {things_draw_recog_df.cat_correct.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mean top 1 recognition accuracy is {things_draw_recog_df.top1_correct.mean()}, \\n\\\n",
    "and mean top 1 accuracy at the category level is {things_draw_recog_df.top1_cat_correct.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### most easily recognized concepts\n",
    "recog_response_vec_cat_df.groupby('concept').mean_accuracy.mean().sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_response_vec_df[recog_response_vec_df.concept=='mustache'].mean_accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_response_vec_df.groupby('concept').mean_accuracy.mean().sort_values(ascending=False).tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### make a plot of the mean accuracy for each concept sorted from highest to lowest using sns barplot\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x='concept',y='mean_accuracy',data=recog_response_vec_df,order=recog_response_vec_df.groupby('concept').mean_accuracy.mean().sort_values(ascending=True).index, errorbar=None,color='gray')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.ylabel('mean recognizability',fontsize=25)\n",
    "plt.xlabel('concepts',fontsize=25)\n",
    "## hide y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=25)\n",
    "# plt.title('mean recognition accuracy across concepts', fontsize=25)\n",
    "# plt.savefig('VSS2023_mean_acc.pdf')\n",
    "plt.savefig('ms_all_concepts_mean_acc.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'these labels were never used \\n:{non_label_concepts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group recog_response_vec_df by concept and aggregate by mean accuracy and sum all the columns in things1854concepts\n",
    "\n",
    "recog_response_vec_df_split = recog_response_vec_df.groupby(['uniqueID','familiar']).agg({'mean_accuracy':'mean', 'mean_top1_accuracy':'mean',\\\n",
    "                                                                           **{ x:'sum' for x in things1854concepts}}).reset_index()\n",
    "\n",
    "recog_response_vec_df_agg = recog_response_vec_df.groupby(['uniqueID']).agg({'mean_accuracy':'mean', 'mean_top1_accuracy':'mean',\\\n",
    "                                                                           **{ x:'sum' for x in things1854concepts}}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the mean top 1 accuracy is:',recog_response_vec_df_agg.mean_top1_accuracy.mean())\n",
    "print('sd:',recog_response_vec_df_agg.mean_top1_accuracy.std(),'\\n','max:',recog_response_vec_df_agg.mean_top1_accuracy.max(),'\\n','min:',recog_response_vec_df_agg.mean_top1_accuracy.min())\n",
    "print('the mean any-match accuracy is:',recog_response_vec_df_agg.mean_accuracy.mean())\n",
    "print('sd:',recog_response_vec_df_agg.mean_accuracy.std(),'\\n','max:',recog_response_vec_df_agg.mean_accuracy.max(),'\\n','min:',recog_response_vec_df_agg.mean_accuracy.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the data into two groups\n",
    "familiar_true = recog_response_vec_df_split[recog_response_vec_df_split['familiar'] == True]['mean_accuracy']\n",
    "familiar_false = recog_response_vec_df_split[recog_response_vec_df_split['familiar'] == False]['mean_accuracy']\n",
    "\n",
    "# Perform a t-test\n",
    "t_stat, p_value = ttest_ind(familiar_true, familiar_false)\n",
    "\n",
    "# Print the results\n",
    "print(f'Mean accuracy (familiar=True): {familiar_true.mean()}')\n",
    "print(f'Standard deviation (familiar=True): {familiar_true.std()}')\n",
    "print(f'Mean accuracy (familiar=False): {familiar_false.mean()}')\n",
    "print(f'Standard deviation (familiar=False): {familiar_false.std()}')\n",
    "print(f'T-statistic: {t_stat}')\n",
    "print(f'P-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_recognizability_dict = things_image_recongizability.groupby('uniqueID')['recognizability'].agg(['min', 'max']).to_dict('index')\n",
    "### note: We now use the recognizability_homonyms variable instead as it is fairer \n",
    "recog_response_vec_df_agg['image_recongizability'] = recog_response_vec_df_agg['uniqueID'].apply(lambda x: things_image_recongizability[things_image_recongizability['uniqueID']==x]['recognizability_homonyms'].values.mean())\n",
    "# recog_response_vec_df_agg['mean_accuracy_normalized'] = recog_response_vec_df_agg.apply(lambda x: mm_normalize(x, min_max_recognizability_dict), axis=1)\n",
    "recog_response_vec_df_agg['mean_accuracy_normalized'] = recog_response_vec_df_agg['mean_accuracy']-recog_response_vec_df_agg['image_recongizability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the number of rows in recog_response_vec_df_agg where mean_accuracy_normalized is > 0\n",
    "recog_response_vec_df_agg[recog_response_vec_df_agg['mean_accuracy_normalized']>0].shape[0]\n",
    "\n",
    "### print the top 30 concepts with the highest mean_accuracy_normalized\n",
    "recog_response_vec_df_agg.sort_values(by='mean_accuracy_normalized',ascending=False).head(30)[['uniqueID','mean_accuracy_normalized','mean_accuracy','image_recongizability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'proportion of sketch advantaged concepts - {np.round(recog_response_vec_df_agg[recog_response_vec_df_agg[\"mean_accuracy_normalized\"]>0].shape[0]/recog_response_vec_df_agg.shape[0],3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### do a fisher transformed t-test to compare mean_accuracy and image_recongizability\n",
    "\n",
    "def safe_arctanh(r):\n",
    "    r = np.clip(r, -0.9999, 0.9999)  # Restrict values within valid range\n",
    "    return np.arctanh(r)\n",
    "\n",
    "\n",
    "drawing_recog = recog_response_vec_df_agg['mean_accuracy'].values\n",
    "image_recog = recog_response_vec_df_agg['image_recongizability'].values\n",
    "\n",
    "drawing_recog_fisher = safe_arctanh(drawing_recog)\n",
    "image_recog_fisher = safe_arctanh(image_recog)\n",
    "\n",
    "t_stat, p_val = ttest_rel( image_recog_fisher,drawing_recog_fisher)\n",
    "print(f\"t-statistic: {t_stat}, p-value: {p_val}\")\n",
    "\n",
    "print(f\"mean recognizability for drawings: {drawing_recog.mean()}\")\n",
    "print(f\"standard deviation recognizability for drawings: {drawing_recog.std()}\")\n",
    "print(f\"mean recognizability for images: {image_recog.mean()}\")\n",
    "print(f\"standard deviation recognizability for images: {image_recog.std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute the correlation between image_recog and drawing_recog and print all the related statistics\n",
    "r, p = pearsonr(image_recog, drawing_recog)\n",
    "print(f\"Pearson r between image recognizability and drawing recognizability: {r}\")\n",
    "print(f\"P-value: {p}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tab20 = sns.color_palette(\"tab20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='image_recongizability',y='mean_accuracy',data=recog_response_vec_df_agg,color=tab20[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0,1],[0,1],color='black',linestyle='--')\n",
    "plt.xlabel('image recognizability',fontsize=25)\n",
    "plt.ylabel('drawing recognizability',fontsize=25)\n",
    "### add some horizontal and vertical lines at .5\n",
    "# plt.axhline(y=.5, color='gray', linestyle='--')\n",
    "# plt.axvline(x=.5, color='gray', linestyle='--')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "plt.savefig('image_vs_drawing_recog.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x='uniqueID',y='mean_accuracy_normalized',data=recog_response_vec_df_agg,order=recog_response_vec_df_agg.groupby('uniqueID').mean_accuracy_normalized.mean().sort_values(ascending=True).index, errorbar=None,color='gray')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.ylabel('photo-relative mean \\n sketch recognizability',fontsize=25)\n",
    "plt.xlabel('concepts',fontsize=25)\n",
    "## hide y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=25)\n",
    "# plt.title('image-relative recognizability scores', fontsize=25)\n",
    "plt.savefig('all_concepts_im_relative_recognizability.pdf')\n",
    "### despine please\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### get the rank of each uniqueID in recog_response_vec_df_agg when sorting by mean_accuracy\n",
    "\n",
    "recog_response_vec_df_agg['normed_acc_rank'] = recog_response_vec_df_agg['mean_accuracy_normalized'].rank(ascending=False)\n",
    "recog_response_vec_df_agg['acc_rank'] = recog_response_vec_df_agg['mean_accuracy'].rank(ascending=False)\n",
    "recog_response_vec_df_agg['image_acc_rank'] = recog_response_vec_df_agg['image_recongizability'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to perform the bootstrapping and compute Kendall Tau\n",
    "def bootstrap_kendall_tau(df, n_iterations=1000, sample_size=20):\n",
    "    kendall_taus = []\n",
    "\n",
    "    for _ in tqdm(range(n_iterations), desc=\"Bootstrapping\"):\n",
    "        # Sample 13 rows for each uniqueID\n",
    "        sample1 = df.groupby('uniqueID').apply(lambda x: x.sample(n=sample_size, replace=True)).reset_index(drop=True)\n",
    "        sample2 = df.groupby('uniqueID').apply(lambda x: x.sample(n=sample_size, replace=True)).reset_index(drop=True)\n",
    "\n",
    "        # Compute the mean recognizability_homonyms for each uniqueID\n",
    "        mean_sample1 = sample1.groupby('uniqueID')['recognizability_homonyms'].mean()\n",
    "        mean_sample2 = sample2.groupby('uniqueID')['recognizability_homonyms'].mean()\n",
    "\n",
    "        # Compute the Kendall Tau coefficient\n",
    "        tau, _ = kendalltau(mean_sample1, mean_sample2)\n",
    "        kendall_taus.append(tau)\n",
    "\n",
    "    return kendall_taus\n",
    "\n",
    "kendall_taus = bootstrap_kendall_tau(things_image_recongizability)\n",
    "\n",
    "# Output the distribution of Kendall Tau coefficients\n",
    "plt.hist(kendall_taus, bins=20, edgecolor='black')\n",
    "plt.xlabel('Kendall Tau Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Kendall Tau Coefficients')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f'Mean Kendall Tau: {np.mean(kendall_taus)}')\n",
    "print(f'Standard Deviation of Kendall Tau: {np.std(kendall_taus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(spearmanr(recog_response_vec_df_agg['image_acc_rank'],recog_response_vec_df_agg['acc_rank']))\n",
    "print(\"correlation between mean drawing recognizability and image recognizability:\")\n",
    "print(kendalltau(recog_response_vec_df_agg['image_acc_rank'],recog_response_vec_df_agg['acc_rank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### noise corrected Kendall tau\n",
    "print(\"noise corrected correlation between mean drawing recognizability and image recognizability:\")\n",
    "print(kendalltau(recog_response_vec_df_agg['image_acc_rank'],recog_response_vec_df_agg['acc_rank'])[0]/np.mean(kendall_taus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute distributional statistics for each concept\n",
    "\n",
    "for i, row in recog_response_vec_df_agg.iterrows():\n",
    "    response_vec = np.array(row[things1854concepts.tolist()])\n",
    "    ### compute the entropy of the response vector\n",
    "    probabilities = response_vec / np.sum(response_vec)\n",
    "    entropy_value = entropy(probabilities.astype(float))\n",
    "    recog_response_vec_df_agg.loc[i,'entropy'] = entropy_value\n",
    "    recog_response_vec_df_agg.loc[i,'sdi'] =utils.SDI(row[things1854concepts.tolist()])\n",
    "    \n",
    "for i, row in recog_response_vec_df_split.iterrows():\n",
    "    response_vec = np.array(row[things1854concepts.tolist()])\n",
    "    ### compute the entropy of the response vector\n",
    "    probabilities = response_vec / np.sum(response_vec)\n",
    "    entropy_value = entropy(probabilities.astype(float))\n",
    "    recog_response_vec_df_split.loc[i,'entropy'] = entropy_value\n",
    "    recog_response_vec_df_split.loc[i,'sdi'] =utils.SDI(row[things1854concepts.tolist()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the correlation between entropy and sdi\n",
    "print(f\"correlation between entropy and sdi across all concepts: \\n {pearsonr(recog_response_vec_df_agg['entropy'].values,recog_response_vec_df_agg['sdi'].values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"correlation between mean accuracy and sdi across all concepts: \\n {pearsonr(recog_response_vec_df_agg['mean_accuracy'].values,recog_response_vec_df_agg['sdi'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"correlation between mean top 1 accuracy and sdi across all concepts: \\n {pearsonr(recog_response_vec_df_agg['mean_top1_accuracy'].values,recog_response_vec_df_agg['sdi'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"correlation between mean top accuracy and entropy across all concepts when observer did not recognize the concept:\\n{pearsonr(recog_response_vec_df_split[recog_response_vec_df_split.familiar==False]['mean_accuracy'].values,recog_response_vec_df_split[recog_response_vec_df_split.familiar==False]['entropy'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_df = recog_response_vec_df_split[recog_response_vec_df_split.familiar==True]\n",
    "unfam_df = recog_response_vec_df_split[recog_response_vec_df_split.familiar==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quart_fam_df = []\n",
    "quart_unfam_df =[]\n",
    "\n",
    "\n",
    "fam_df['quartile_id'] = pd.qcut(fam_df['mean_top1_accuracy'],10,labels=False,duplicates='drop')\n",
    "# unfam_df['quartile_id'] = pd.qcut(unfam_df['mean_top1_accuracy'],10,labels=False,duplicates='drop')\n",
    "unfam_df['quartile_id']=unfam_df['uniqueID'].apply(lambda x: fam_df[fam_df.uniqueID == x]['quartile_id'].values[0])\n",
    "\n",
    "recog_response_vec_df_agg['quartile_id'] = recog_response_vec_df_agg['uniqueID'].apply(lambda x: fam_df[fam_df.uniqueID == x]['quartile_id'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x='quartile_id', y='mean_top1_accuracy', data=recog_response_vec_df_agg.groupby(['quartile_id']).mean_top1_accuracy.mean().sort_values(ascending=False).reset_index(),errorbar=None,linewidth=3,color=tab20[0])\n",
    "\n",
    "plt.ylabel('Mean Accuracy',fontsize=25)\n",
    "plt.xlabel('Decile',fontsize=25)\n",
    "## hide y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=25)\n",
    "plt.ylim(0,1)\n",
    "# plt.title('Mean Recognition Accuracy', fontsize=25)\n",
    "plt.savefig('VSS2023_mean_acc_line.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab20 = sns.color_palette(\"tab20\")\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.lineplot(x='quartile_id', y='mean_top1_accuracy', data=fam_df.groupby(['quartile_id']).mean_top1_accuracy.mean().sort_values(ascending=False).reset_index(),errorbar=None,linewidth=3,color=tab20[0])\n",
    "plt.ylabel('Mean Accuracy',fontsize=25)\n",
    "plt.xlabel('Decile',fontsize=25)\n",
    "## hide y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=25)\n",
    "plt.ylim(0,1)\n",
    "# plt.title('Mean Recognition Accuracy', fontsize=25)\n",
    "plt.savefig('VSS2023_mean_acc_line_split.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x='quartile_id', y='mean_top1_accuracy', data=fam_df.groupby(['quartile_id']).mean_top1_accuracy.mean().sort_values(ascending=False).reset_index(),errorbar=None,linewidth=3,color=tab20[0])\n",
    "sns.lineplot(x='quartile_id', y='mean_top1_accuracy', data=unfam_df.groupby(['quartile_id']).mean_top1_accuracy.mean().sort_values(ascending=False).reset_index(),errorbar=None,linewidth=3,color=tab20[1])\n",
    "\n",
    "plt.ylabel('Mean Accuracy',fontsize=25)\n",
    "plt.xlabel('Decile',fontsize=25)\n",
    "## hide y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=25)\n",
    "plt.ylim(0,1)\n",
    "# plt.title('Mean Recognition Accuracy', fontsize=25)\n",
    "plt.savefig('VSS2023_mean_acc_line_split.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x='uniqueID',y='sdi',data=recog_response_vec_df_agg,order=recog_response_vec_df_agg.groupby('uniqueID').sdi.mean().sort_values(ascending=False).index, errorbar=None,color='gray')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.ylabel('SDI',fontsize=25)\n",
    "plt.xlabel('Concept',fontsize=25)\n",
    "## hide y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=25)\n",
    "plt.ylim(.5,1)\n",
    "plt.title('SDI', fontsize=25)\n",
    "plt.savefig('VSS2023_SDI.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x='quartile_id', y='sdi', data=fam_df.groupby(['quartile_id']).sdi.mean().sort_values(ascending=False).reset_index(),errorbar=None,linewidth=3,color=tab20[0])\n",
    "sns.lineplot(x='quartile_id', y='sdi', data=unfam_df.groupby(['quartile_id']).sdi.mean().sort_values(ascending=False).reset_index(),errorbar=None,linewidth=3,color=tab20[1])\n",
    "\n",
    "plt.ylabel('SDI',fontsize=25)\n",
    "plt.xlabel('Decile',fontsize=25)\n",
    "## hide y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=25)\n",
    "plt.ylim(.5,1)\n",
    "# plt.title('Mean Recognition Accuracy', fontsize=25)\n",
    "plt.savefig('VSS2023_SDI_split.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='sdi',y='mean_accuracy',data=recog_response_vec_df_agg, color = tab20[1])\n",
    "plt.xlabel(\"Simpson's diversity index\",fontsize=20)\n",
    "plt.ylabel('drawing recognizability',fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "# plt.savefig('VSS2023_SDI_v_accuracy.pdf')\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig('SDI_v_mean_accuracy.pdf')\n",
    "# plt.suptitle('Mean Overall Accuracy vs SDI',fontsize=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using seaborn make a scatterplot of mean_accuracy vs sdi in recog_response_vec_df_agg\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='sdi',y='mean_top1_accuracy',data=recog_response_vec_df_agg, color=tab20[1])\n",
    "plt.xlabel('SDI',fontsize=20)\n",
    "plt.ylabel('Mean Top 1 Accuracy',fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "# plt.title('Mean Top1 Accuracy vs SDI',fontsize=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_response_vec_df_agg['mean_fam'] = mean_fam_df['recog']\n",
    "pearsonr(recog_response_vec_df_agg['mean_fam'].values,recog_response_vec_df_agg['mean_accuracy'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x='uniqueID',y='mean_accuracy_normalized',data=recog_response_vec_df_agg,order=recog_response_vec_df_agg.groupby('uniqueID').mean_accuracy_normalized.mean().sort_values(ascending=True).index, errorbar=None,color='gray')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.ylabel('photo-relative mean \\n sketch recognizability',fontsize=25)\n",
    "plt.xlabel('concepts',fontsize=25)\n",
    "## hide y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=25)\n",
    "# plt.title('image-relative recognizability scores', fontsize=25)\n",
    "plt.savefig('all_concepts_im_relative_recognizability.pdf')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### get the rank of each uniqueID in recog_response_vec_df_agg when sorting by mean_accuracy\n",
    "\n",
    "recog_response_vec_df_agg['normed_acc_rank'] = recog_response_vec_df_agg['mean_accuracy_normalized'].rank(ascending=False)\n",
    "recog_response_vec_df_agg['acc_rank'] = recog_response_vec_df_agg['mean_accuracy'].rank(ascending=False)\n",
    "recog_response_vec_df_agg['image_acc_rank'] = recog_response_vec_df_agg['image_recongizability'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using seaborn make a scatterplot of mean_accuracy vs sdi in recog_response_vec_df_agg\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='mean_fam',y='mean_accuracy',data=recog_response_vec_df_agg)\n",
    "plt.xlabel('Mean Familiarity',fontsize=20)\n",
    "plt.ylabel('Mean Accuracy',fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.title('Mean Overall Accuracy vs Familiarity',fontsize=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quart_recog_df = []\n",
    "for this_concept in things1854concepts:\n",
    "    ds = recog_response_vec_df[recog_response_vec_df.uniqueID==this_concept]\n",
    "    ds['quartile_id'] = pd.qcut(ds['num_strokes'],4,labels=False,duplicates='drop')\n",
    "\n",
    "    ds_g = ds.groupby(['uniqueID','quartile_id'])[['num_strokes','mean_top1_accuracy','mean_accuracy']].mean().reset_index()\n",
    "    quart_recog_df.append(ds_g)\n",
    "    \n",
    "\n",
    "## divide t into 4 quartiles based on num_strokes\n",
    "\n",
    "\n",
    "quart_recog_df = pd.concat(quart_recog_df).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpt_feat_cols = things_gpt_word_embeds.columns[:-1]\n",
    "\n",
    "\n",
    "# cosine_dist_df = pd.DataFrame(cosine_distances(things_gpt_word_embeds[gpt_feat_cols]), columns=things_gpt_word_embeds['concept'].unique(), index=things_gpt_word_embeds['concept'].unique())\n",
    "\n",
    "cosine_dist_df = pd.DataFrame(cosine_distances(spose_embeds[spose_cols]), columns=things1854concepts, index=things1854concepts)\n",
    "\n",
    "nearest_concepts = {}\n",
    "for concept in cosine_dist_df.columns:\n",
    "    nearest_concepts[concept] = cosine_dist_df[concept].sort_values()[1:].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_draw_recog_df['top1_rank'] = \\\n",
    "    things_draw_recog_df.apply(lambda x: 0 if x.response_list[0]==x.uniqueID else nearest_concepts[x.uniqueID].index(x.response_list[0]),axis=1)\n",
    "\n",
    "things_draw_recog_df['topk_rank'] = \\\n",
    "    things_draw_recog_df.apply(\n",
    "        lambda x: 0 if any(response == x.uniqueID for response in x.response_list) else min(\n",
    "            (nearest_concepts[x.uniqueID].index(this_response) for this_response in x.response_list if this_response in nearest_concepts[x.uniqueID]),\n",
    "            default=None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_image_recongizability['response_list'] =things_image_recongizability['answer'].apply(lambda x: x.split(', '))\n",
    "\n",
    "\n",
    "# Explode the 'response_list' column to create a new row for each item in the list\n",
    "things_image_recongizability_trialwise = things_image_recongizability.explode('response_list')\n",
    "\n",
    "\n",
    "# Rename the 'response_list' column to 'response'\n",
    "things_image_recongizability_trialwise = things_image_recongizability_trialwise.rename(columns={'response_list': 'response'})\n",
    "\n",
    "# Reset the index\n",
    "things_image_recongizability_trialwise = things_image_recongizability_trialwise.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "display_label2uniqueID = dict(zip(things_image_recongizability_trialwise.display_label,things_image_recongizability_trialwise.uniqueID))\n",
    "\n",
    "# things_image_recongizability_trialwise['response'] = things_image_recongizability_trialwise['response'].apply(lambda x: \"baton (conductor's baton)\" if x == '\"baton (conductors baton)\"' else x)\n",
    "# things_image_recongizability_trialwise['response'] = things_image_recongizability_trialwise['response'].apply(lambda x: \"tick (insect)\" if x == '<em class=\"Highlight ht22e5d362-63a9-4678-acc3-95564bce8064\" highlight=\"true\" match=\"tick\" loopnumber=\"807296746\" style=\"font-style: inherit;\">tick</em> (insect)' else x)\n",
    "# # things_image_recongizability_trialwise = things_image_recongizability_trialwise[things_image_recongizability_trialwise.response!='229'].reset_index(drop=True)\n",
    "# things_image_recongizability_trialwise = things_image_recongizability_trialwise[things_image_recongizability_trialwise.response!='229) 1px 1px; border-radius: 3px; background-color: rgb(255'].reset_index(drop=True)\n",
    "\n",
    "### if response is not among the keys in display_label2uniqueID remove the row and reset index\n",
    "things_image_recongizability_trialwise = things_image_recongizability_trialwise[things_image_recongizability_trialwise.response.isin(display_label2uniqueID.keys())].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_label2uniqueID['tick (insect)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "things_image_recongizability_trialwise['topk_rank'] = \\\n",
    "    things_image_recongizability_trialwise.apply(\n",
    "        lambda x: 0 if x.response == x.display_label else nearest_concepts[display_label2uniqueID[x.display_label]].index(display_label2uniqueID[x.response]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_draw_recog_df[~(things_draw_recog_df['topk_rank']==things_draw_recog_df['top1_rank'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### iterate over all the recognition trials and compute the similarity between the target concept and the mean of the responses\n",
    "for i, row in things_draw_recog_df.iterrows():\n",
    "    target_embed = spose_embeds[spose_embeds['concept']==row.uniqueID][spose_cols].values[0]\n",
    "    responses = row.response_list\n",
    "    response_embeds = []\n",
    "    for this_response in responses:\n",
    "        response_embed = spose_embeds[spose_embeds['concept']==this_response][spose_cols].values[0]\n",
    "        response_embeds.append(response_embed)\n",
    "\n",
    "    similarity_scores = [cosine_similarity(target_embed.reshape(1, -1),this_embed.reshape(1, -1))[0][0] for this_embed in response_embeds]\n",
    "    things_draw_recog_df.loc[i,'similarity_score'] = max(similarity_scores)\n",
    "    mean_response_embed = np.mean(response_embeds,axis=0)\n",
    "    mean_similarity_score = cosine_similarity(target_embed.reshape(1, -1),mean_response_embed.reshape(1, -1))\n",
    "    things_draw_recog_df.loc[i,'similarity_score_mean'] = mean_similarity_score[0][0]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for things_draw_recog_df first groupby concept then summarize then make a plot of mean_similarity_score vs. mean_accuracy\n",
    "\n",
    "things_draw_recog_df_agg =  things_draw_recog_df.groupby('uniqueID').agg({'similarity_score':'mean','correct':'mean','similarity_score_mean':'mean'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(things_draw_recog_df_agg['similarity_score'],things_draw_recog_df_agg['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things_draw_recog_df[['uniqueID','similarity_score','correct']].to_csv(os.path.join(data_dir,'things_draw_recog_similarity.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(things_draw_recog_df_agg['similarity_score_mean'],things_draw_recog_df_agg['correct'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## make a plot of similarity_score vs. correct in things_draw_recog_df_agg\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='similarity_score',y='correct',data=things_draw_recog_df_agg,color=tab20[1])\n",
    "plt.xlabel('mean semantic similarity between guesses and labels',fontsize=20)\n",
    "plt.ylabel('drawing recognizability',fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(.3,1)\n",
    "# plt.title('Mean Accuracy vs. Similarity Score',fontsize=25)\n",
    "sns.despine()\n",
    "plt.savefig('accuracy_vs_similarity.pdf',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.histplot(data=things_draw_recog_df[things_draw_recog_df['top1_rank']!=0],x='top1_rank', bins=30,legend=False,stat='probability')\n",
    "# plt.title('Top 1 Rank Distribution')\n",
    "plt.xlabel('Top 1 Rank', fontsize=20)\n",
    "plt.ylabel('Proportion', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "## set facecolor to be white\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.histplot(data=things_draw_recog_df[things_draw_recog_df['topk_rank']!=0],x='top1_rank', bins=30,legend=False,stat='probability')\n",
    "# plt.title('Top 1 Rank Distribution')\n",
    "plt.xlabel('Top k Rank', fontsize=20)\n",
    "plt.ylabel('Proportion', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "## set facecolor to be white\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.histplot(data=things_draw_recog_df[(things_draw_recog_df['top1_rank']!=0) &(things_draw_recog_df['concept']=='pom-pom')],x='top1_rank', bins=30,legend=False,stat='probability',color=tab20[1])\n",
    "plt.title('Pom-pom', fontsize=25)\n",
    "plt.xlabel('Rank', fontsize=20)\n",
    "plt.ylabel('Proportion', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlim(0,1850)\n",
    "## set facecolor to be white\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "plt.savefig('VSS2023_rank_dist_pompom.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import gamma\n",
    "\n",
    "\n",
    "# unif_df = pd.DataFrame({'x':np.arange(1854),'y':np.ones(1854)})\n",
    "\n",
    "# xs = []\n",
    "# # ys = []\n",
    "# for i in range(1,1854):\n",
    "#     this_y = 2/i*10000\n",
    "#     for j in range(int(this_y)):\n",
    "#         xs.append(i)\n",
    "# exp_df = pd.DataFrame({'x':xs})\n",
    "\n",
    "# plt.figure(figsize=(15,8))\n",
    "# sns.histplot(data=unif_df,x='x', bins=40,legend=False,stat='probability',color=tab20[1])\n",
    "# plt.title('Uniform', fontsize=25)\n",
    "# plt.xlabel('Rank', fontsize=20)\n",
    "# plt.ylabel('Proportion', fontsize=20)\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks([],fontsize=18)\n",
    "# plt.xlim(0,1850)\n",
    "# plt.ylim(0,.06)\n",
    "# ## set facecolor to be white\n",
    "# ax = plt.gca()\n",
    "# ax.set_facecolor('white')\n",
    "# plt.savefig('VSS2023_uniform.pdf')\n",
    "\n",
    "# plt.figure(figsize=(15,8))\n",
    "# sns.histplot(data=exp_df,x='x', bins=40,legend=False,stat='probability',color=tab20[1])\n",
    "# plt.title('Skew', fontsize=25)\n",
    "# plt.xlabel('Rank', fontsize=20)\n",
    "# plt.ylabel('Proportion', fontsize=20)\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks([],fontsize=18)\n",
    "# plt.xlim(0,1850)\n",
    "# plt.ylim(0,.06)\n",
    "# ## set facecolor to be white\n",
    "# ax = plt.gca()\n",
    "# ax.set_facecolor('white')\n",
    "# plt.savefig('VSS2023_left_skew.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Recognition performance to other measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THINGS_mem_dict = dict(zip(THINGS_mem['concept'],THINGS_mem['CR']))\n",
    "### fix\n",
    "# THINGS_mem_dict['goalpost'] = 0\n",
    "things_draw_recog_df['typicality'] = things_draw_recog_df['uniqueID'].map(THINGS_mem_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### fit a mixed effects model predicting similarity_score from mean_mem and num_strokes with random intercepts for subject \n",
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# reg_df = things_draw_recog_df.dropna(subset=['typicality','similarity_score','num_strokes'])\n",
    "\n",
    "# things_draw_recog_df[\"workerID\"] = things_draw_recog_df[\"workerID\"].astype(str)\n",
    "# md = smf.mixedlm(\"similarity_score ~ typicality + num_strokes \", reg_df, groups=reg_df[\"workerID\"])\n",
    "# mdf = md.fit()\n",
    "# print(mdf.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.corrcoef(reg_df['similarity_score'],reg_df['typicality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_list_top1_rank=[]\n",
    "concept_list_topk_rank=[]\n",
    "top1_ranks = []\n",
    "topk_ranks = []\n",
    "cumulative_props_top1=[]\n",
    "cumulative_props_topk=[]\n",
    "\n",
    "\n",
    "for this_concept in things1854concepts:\n",
    "    ds = things_draw_recog_df[things_draw_recog_df['uniqueID']==this_concept]\n",
    "    ds = ds[ds.top1_rank!=0] ### get all the rows where the guess was not the correct object concept\n",
    "    top1_ranks.append(ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().index)\n",
    "    topk_ranks.append(ds['topk_rank'].value_counts(normalize=True).sort_index().cumsum().index)\n",
    "    cumulative_props_top1.append(ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().values)\n",
    "    cumulative_props_topk.append(ds['topk_rank'].value_counts(normalize=True).sort_index().cumsum().values)\n",
    "    concept_list_top1_rank.append([this_concept]*ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "    concept_list_topk_rank.append([this_concept]*ds['topk_rank'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "\n",
    "top1_ranks = np.concatenate(top1_ranks)\n",
    "cumulative_props_top1 = np.concatenate(cumulative_props_top1)\n",
    "topk_ranks = np.concatenate(topk_ranks)\n",
    "cumulative_props_topk = np.concatenate(cumulative_props_topk)\n",
    "concept_list_top1_rank = np.concatenate(concept_list_top1_rank)\n",
    "concept_list_topk_rank = np.concatenate(concept_list_topk_rank)\n",
    "\n",
    "cumulative_prop_df_top1 = pd.DataFrame({'concept':concept_list_top1_rank,'rank':top1_ranks,'cumulative_prop':cumulative_props_top1})\n",
    "cumulative_prop_df_top1['rank'] = cumulative_prop_df_top1['rank']/1854\n",
    "\n",
    "cumulative_prop_df_topk = pd.DataFrame({'concept':concept_list_topk_rank,'rank':topk_ranks,'cumulative_prop':cumulative_props_topk})\n",
    "cumulative_prop_df_topk['rank'] = cumulative_prop_df_topk['rank']/1854\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# concept_list_topk_rank=[]\n",
    "# topk_ranks = []\n",
    "# cumulative_props_topk=[]\n",
    "\n",
    "\n",
    "# for this_concept in things1854concepts:\n",
    "#     ds = things_image_recongizability[things_image_recongizability['uniqueID']==this_concept]\n",
    "#     ds = ds[ds.top1_rank!=0] ### get all the rows where the guess was not the correct object concept\n",
    "#     topk_ranks.append(ds['topk_rank'].value_counts(normalize=True).sort_index().cumsum().index)\n",
    "#     cumulative_props_topk.append(ds['topk_rank'].value_counts(normalize=True).sort_index().cumsum().values)\n",
    "\n",
    "#     concept_list_topk_rank.append([this_concept]*ds['topk_rank'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "\n",
    "# topk_ranks = np.concatenate(topk_ranks)\n",
    "# cumulative_props_topk = np.concatenate(cumulative_props_topk)\n",
    "# concept_list_topk_rank = np.concatenate(concept_list_topk_rank)\n",
    "\n",
    "\n",
    "# cumulative_prop_df_topk_images = pd.DataFrame({'concept':concept_list_topk_rank,'rank':topk_ranks,'cumulative_prop':cumulative_props_topk})\n",
    "# cumulative_prop_df_topk_images['rank'] = cumulative_prop_df_topk_images['rank']/1854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cumulative_props(df, concepts, rank_column, normalize_factor):\n",
    "    concept_list = []\n",
    "    ranks = []\n",
    "    cumulative_props = []\n",
    "\n",
    "    for this_concept in concepts:\n",
    "        ds = df[df['uniqueID'] == this_concept]\n",
    "        ds = ds[ds[rank_column] != 0]  # Get all the rows where the guess was not the correct object concept\n",
    "        rank_counts = ds[rank_column].value_counts(normalize=True).sort_index().cumsum()\n",
    "        ranks.append(rank_counts.index)\n",
    "        cumulative_props.append(rank_counts.values)\n",
    "        concept_list.append([this_concept] * rank_counts.index.shape[0])\n",
    "\n",
    "    ranks = np.concatenate(ranks)\n",
    "    cumulative_props = np.concatenate(cumulative_props)\n",
    "    concept_list = np.concatenate(concept_list)\n",
    "\n",
    "    cumulative_prop_df = pd.DataFrame({'concept': concept_list, 'rank': ranks, 'cumulative_prop': cumulative_props})\n",
    "    cumulative_prop_df['rank'] = cumulative_prop_df['rank'] / normalize_factor\n",
    "\n",
    "    return cumulative_prop_df\n",
    "\n",
    "# Define the concepts and normalize factor\n",
    "concepts = things1854concepts\n",
    "normalize_factor = 1854\n",
    "\n",
    "# Compute cumulative properties for top1_rank and topk_rank for drawings\n",
    "cumulative_prop_df_top1_drawings = compute_cumulative_props(things_draw_recog_df, concepts, 'top1_rank', normalize_factor)\n",
    "cumulative_prop_df_topk_drawings = compute_cumulative_props(things_draw_recog_df, concepts, 'topk_rank', normalize_factor)\n",
    "\n",
    "# Compute cumulative properties for topk_rank for images\n",
    "cumulative_prop_df_topk_images = compute_cumulative_props(things_image_recongizability_trialwise, concepts, 'topk_rank', normalize_factor)\n",
    "\n",
    "# Print the resulting dataframes\n",
    "print(cumulative_prop_df_top1_drawings.head())\n",
    "print(cumulative_prop_df_topk_drawings.head())\n",
    "print(cumulative_prop_df_topk_images.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(10,figsize=(10,10))\n",
    "sns.lineplot(data=cumulative_prop_df_top1_drawings,x='rank',y='cumulative_prop',hue='concept',legend=False)\n",
    "plt.title('Cumulative Proportion of Top 1 Rank',fontsize=25)\n",
    "plt.xlabel('Rank', fontsize=20)\n",
    "plt.ylabel('Cumulative Proportion', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(10,figsize=(10,10))\n",
    "sns.lineplot(data=cumulative_prop_df_top1_drawings,x='rank',y='cumulative_prop',legend=False)\n",
    "# plt.title('Proportion ',fontsize=25)\n",
    "plt.xlabel('Semantic distance of top1 guesses', fontsize=20)\n",
    "plt.ylabel('Cumulative Proportion', fontsize=20)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.plot(np.arange(0,1,.002),np.arange(0,1,.002),linewidth=2,color='gray',linestyle='--')\n",
    "# plt.savefig('overall_AUC.pdf',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(10,figsize=(10,10))\n",
    "sns.lineplot(data=cumulative_prop_df_topk_drawings,x='rank',y='cumulative_prop',legend=False)\n",
    "# plt.title('Proportion ',fontsize=25)\n",
    "plt.xlabel('Semantic distance of top-k guesses', fontsize=20)\n",
    "plt.ylabel('Cumulative Proportion', fontsize=20)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.plot(np.arange(0,1,.002),np.arange(0,1,.002),linewidth=2,color='gray',linestyle='--')\n",
    "# plt.savefig('overall_AUC.pdf',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(columns = ['concept','auc_top1_drawings','auc_topk_drawings','auc_top_k_images'])\n",
    "for this_concept in things1854concepts:\n",
    "    this_df_top1_drawings = cumulative_prop_df_top1_drawings[cumulative_prop_df_top1_drawings['concept']==this_concept]\n",
    "    this_df_topk_drawings = cumulative_prop_df_topk_drawings[cumulative_prop_df_topk_drawings['concept']==this_concept]\n",
    "    this_df_topk_images = cumulative_prop_df_topk_images[cumulative_prop_df_topk_images['concept']==this_concept]\n",
    "    this_df_top1_drawings = pd.concat([pd.DataFrame({'concept':this_concept,'rank':0,'cumulative_prop':0},index=[0]),\n",
    "                                       this_df_top1_drawings,\n",
    "                                       pd.DataFrame({'concept':this_concept,'rank':1,'cumulative_prop':1},index=[this_df_top1_drawings.shape[0]+1])])\n",
    "    this_df_topk_drawings = pd.concat([pd.DataFrame({'concept':this_concept,'rank':0,'cumulative_prop':0},index=[0]),\n",
    "                                       this_df_topk_drawings,\n",
    "                                       pd.DataFrame({'concept':this_concept,'rank':1,'cumulative_prop':1},index=[this_df_topk_drawings.shape[0]+1])])\n",
    "    this_df_topk_images = pd.concat([pd.DataFrame({'concept':this_concept,'rank':0,'cumulative_prop':0},index=[0]),\n",
    "                                     this_df_topk_images,\n",
    "                                     pd.DataFrame({'concept':this_concept,'rank':1,'cumulative_prop':1},index=[this_df_topk_images.shape[0]+1])])\n",
    "    this_auc_top1_drawings = this_df_top1_drawings.groupby('concept').apply(lambda x: auc(x['rank'],x['cumulative_prop']))\n",
    "    this_auc_topk_drawings = this_df_topk_drawings.groupby('concept').apply(lambda x: auc(x['rank'],x['cumulative_prop']))\n",
    "    this_auc_topk_images = this_df_topk_images.groupby('concept').apply(lambda x: auc(x['rank'],x['cumulative_prop']))\n",
    "    auc_df = pd.concat([auc_df,pd.DataFrame({'concept':this_concept,'auc_top1_drawings':this_auc_top1_drawings.values[0], 'auc_topk_drawings':this_auc_topk_drawings.values[0], 'auc_topk_images':this_auc_topk_images.values[0] },index=[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(10,figsize=(10,10))\n",
    "# sns.lineplot(data=cum_prop_df[cum_prop_df.concept=='camel'],x='rank',y='cum_prop',legend=False, linewidth=5)\n",
    "# plt.title(f'Camel AUC: {0.98}',fontsize=25)\n",
    "# plt.xlabel('Semantic distance of guesses', fontsize=20)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "# plt.tick_params(labelsize=20)\n",
    "# plt.ylabel('Cumulative Proportion', fontsize=20)\n",
    "# plt.plot(np.arange(0,1,.002),np.arange(0,1,.002),linewidth=2,color='gray',linestyle='--')\n",
    "# plt.savefig('camel_AUC.pdf',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(10,figsize=(10,10))\n",
    "# sns.lineplot(data=cum_prop_df[cum_prop_df.concept=='bagpipe'],x='rank',y='cum_prop',legend=False, linewidth=5)\n",
    "# plt.title(f'Bagpipe AUC: {0.43}',fontsize=25)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "# plt.xlabel('Semantic distance of guesses', fontsize=20)\n",
    "# plt.ylabel('Cumulative Proportion', fontsize=20)\n",
    "# plt.tick_params(labelsize=20)\n",
    "# plt.plot(np.arange(0,1,.002),np.arange(0,1,.002),linewidth=2,color='gray',linestyle='--')\n",
    "# plt.savefig('bagpipe_AUC.pdf',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### count how many rows have auc_topk> .5\n",
    "auc_df.auc_topk_images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the data and the variables to plot\n",
    "data = auc_df\n",
    "variables = [\n",
    "    ('auc_top1_drawings', 'Drawings (top 1 guesses)'),\n",
    "    ('auc_topk_drawings', 'Drawings (top k guesses)'),\n",
    "    ('auc_topk_images', 'Images (top k guesses)')\n",
    "]\n",
    "\n",
    "# Create a long-form DataFrame for FacetGrid\n",
    "long_df = pd.melt(data, value_vars=[v[0] for v in variables], \n",
    "                  var_name='measure', value_name='value')\n",
    "\n",
    "# Map the measure names to the appropriate labels\n",
    "measure_labels = {v[0]: v[1] for v in variables}\n",
    "long_df['measure'] = long_df['measure'].map(measure_labels)\n",
    "\n",
    "# Create the FacetGrid\n",
    "g = sns.FacetGrid(long_df, col='measure', sharex=True, sharey=True, height=10, aspect=0.8)\n",
    "g.map_dataframe(sns.violinplot, y='value', saturation=1, cut=0, bw_method=.1, inner='quartile', color=tab20[1])\n",
    "g.set(ylim=(0, 1))\n",
    "\n",
    "# Set axis labels and titles\n",
    "g.set_titles(col_template='{col_name}', size=30)\n",
    "\n",
    "\n",
    "# Set the background color for each axis\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_facecolor('white')\n",
    "    ax.tick_params(axis='y', labelsize=30)\n",
    "    mean_auc = np.mean(long_df[long_df['measure'] == ax.get_title()]['value'])\n",
    "    std_auc = np.std(long_df[long_df['measure'] == ax.get_title()]['value'])\n",
    "    n = len(long_df[long_df['measure'] == ax.get_title()]['value'])\n",
    "    ci = 1.96 * (std_auc / np.sqrt(n))\n",
    "    ax.errorbar(x=0, y=mean_auc, yerr=ci, fmt='o', color='black', ecolor='black', capsize=5, markersize=10)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', zorder=999, linewidth=2)\n",
    "    ax.yaxis.set_tick_params(left=True, labelleft=True)  # Ensure y-axis ticks and labels \n",
    "    ax.set_ylabel('semantic neighbor preference', fontsize=35)\n",
    "    ax.set_ylim(0.3, 1)\n",
    "\n",
    "# Adjust the spacing between plots\n",
    "g.figure.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('SNP_violin_facet.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##print relevant stats\n",
    "print(f\"mean auc_top1_drawings: {np.mean(auc_df['auc_top1_drawings'])}\")\n",
    "print(f\"mean auc_topk_drawings: {np.mean(auc_df['auc_topk_drawings'])}\")\n",
    "print(f\"mean auc_topk_images: {np.mean(auc_df['auc_topk_images'])}\")\n",
    "\n",
    "print(f\"std auc_top1_drawings: {np.std(auc_df['auc_top1_drawings'])}\")\n",
    "print(f\"std auc_topk_drawings: {np.std(auc_df['auc_topk_drawings'])}\")\n",
    "print(f\"std auc_topk_images: {np.std(auc_df['auc_topk_images'])}\")\n",
    "\n",
    "### find the correlation between auc_top1_drawings and auc_topk_drawings and print relevant stats\n",
    "\n",
    "print(f\"correlation between auc_top1_drawings and auc_topk_drawings: {pearsonr(auc_df['auc_top1_drawings'],auc_df['auc_topk_drawings'])}\")\n",
    "\n",
    "print(f\"correlation between auc_top1_drawings and auc_topk_images: {pearsonr(auc_df['auc_top1_drawings'],auc_df['auc_topk_images'])}\")\n",
    "\n",
    "print(f\"correlation between auc_topk_drawings and auc_topk_images: {pearsonr(auc_df['auc_topk_drawings'],auc_df['auc_topk_images'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_df['memorability'] = auc_df['concept'].apply(lambda x: THINGS_mem[THINGS_mem.concept==x]['CR'].values[0])\n",
    "auc_df['typicality'] = auc_df['concept'].apply(lambda x: THINGS_mem[THINGS_mem.concept==x]['dim_based_typ'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data and the variables to plot\n",
    "data = auc_df\n",
    "variables = [\n",
    "    ('auc_top1_drawings', 'Drawings (top 1 guesses)'),\n",
    "    ('auc_topk_drawings', 'Drawings (top k guesses)'),\n",
    "    ('auc_topk_images', 'Images (top k guesses)')\n",
    "]\n",
    "\n",
    "# Create a long-form DataFrame for FacetGrid\n",
    "long_df = pd.melt(data, id_vars=['typicality'], value_vars=[v[0] for v in variables], \n",
    "                  var_name='measure', value_name='value')\n",
    "\n",
    "# Map the measure names to the appropriate labels\n",
    "measure_labels = {v[0]: v[1] for v in variables}\n",
    "long_df['measure'] = long_df['measure'].map(measure_labels)\n",
    "\n",
    "# Create the FacetGrid\n",
    "g = sns.FacetGrid(long_df, col='measure', sharex=True, sharey=True, height=8, aspect=1)\n",
    "g.map_dataframe(sns.regplot, x='typicality', y='value', line_kws={'color':'gray'}, scatter_kws={'color':tab20[1]})\n",
    "\n",
    "# Set axis labels and titles\n",
    "# g.set_axis_labels('typicality', 'semantic neighbour preference',size=20)\n",
    "g.set_titles(col_template='{col_name}', size=30)\n",
    "\n",
    "# Set the x limits to 0,1\n",
    "g.set(xlim=(.3, 1))\n",
    "\n",
    "# Format the axes\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_facecolor('white')\n",
    "    ax.tick_params(axis='both', labelsize=30)  # Set both x and y tick label sizes\n",
    "    ax.yaxis.set_tick_params(left=True, labelleft=True)  # Ensure y-axis ticks and labels are visible\n",
    "    ax.set_ylabel('semantic neighbor preference', fontsize=35)\n",
    "    ax.set_xlabel('typicality', fontsize=35)\n",
    "\n",
    "# Adjust the spacing between plots\n",
    "g.figure.subplots_adjust(wspace=0.2)  # Increased wspace slightly to accommodate y-axis labels\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('typicality_vs_snp_facet.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute and print Pearson correlation coefficients\n",
    "for var, label in variables:\n",
    "    r, p = pearsonr(data['typicality'], data[var])\n",
    "    print(f'Pearson correlation between typicality and {label}: r={r:.2f}, p={p:.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data and the variables to plot\n",
    "data = auc_df\n",
    "variables = [\n",
    "    ('auc_top1_drawings', 'Drawings (top 1 guesses)'),\n",
    "    ('auc_topk_drawings', 'Drawings (top k guesses)'),\n",
    "    ('auc_topk_images', 'Images (top k guesses)')\n",
    "]\n",
    "\n",
    "# Create a long-form DataFrame for FacetGrid\n",
    "long_df = pd.melt(data, id_vars=['memorability'], value_vars=[v[0] for v in variables], \n",
    "                  var_name='measure', value_name='value')\n",
    "\n",
    "# Map the measure names to the appropriate labels\n",
    "measure_labels = {v[0]: v[1] for v in variables}\n",
    "long_df['measure'] = long_df['measure'].map(measure_labels)\n",
    "\n",
    "# Create the FacetGrid\n",
    "g = sns.FacetGrid(long_df, col='measure', sharex=True, sharey=True, height=8, aspect=1)\n",
    "g.map_dataframe(sns.regplot, x='memorability', y='value', line_kws={'color':'gray'}, scatter_kws={'color':tab20[1]})\n",
    "\n",
    "# Set axis labels and titles\n",
    "g.set_axis_labels('memorability', 'semantic neighbour preference', size=20)\n",
    "g.set_titles(col_template='{col_name}', size=30)\n",
    "\n",
    "# Set the x limits and ticks\n",
    "x_ticks = np.arange(0.6, 1.01, 0.1)  # Generate ticks from 0.6 to 1.0 in steps of 0.1\n",
    "g.set(xlim=(0.6, 1), xticks=x_ticks)\n",
    "\n",
    "# Format the axes\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_facecolor('white')\n",
    "    ax.tick_params(axis='both', labelsize=30)  # Set both x and y tick label sizes\n",
    "    ax.yaxis.set_tick_params(left=True, labelleft=True)  # Ensure y-axis ticks and labels are visible\n",
    "    # Format x-axis tick labels to show only one decimal place\n",
    "    ax.set_xticklabels([f'{x:.1f}' for x in x_ticks])\n",
    "    ax.set_ylabel('semantic neighbor preference', fontsize=35)\n",
    "    ax.set_xlabel('memorability', fontsize=35)\n",
    "\n",
    "\n",
    "# Adjust the spacing between plots\n",
    "g.figure.subplots_adjust(wspace=0.2)  # Increased wspace slightly to accommodate y-axis labels\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('memorability_vs_snp_facet.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute and print Pearson correlation coefficients\n",
    "for var, label in variables:\n",
    "    # Create mask for non-nan values\n",
    "    mask = ~np.isnan(data['memorability'].values) & ~np.isnan(data[var].values)\n",
    "    \n",
    "    # Calculate correlation using masked values\n",
    "    r, p = pearsonr(data['memorability'].values[mask], \n",
    "                    data[var].values[mask])\n",
    "    \n",
    "    print(f'Pearson correlation between memorability and {label}: r={r:.2f}, p={p:.2e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things1854concepts_str = np.vectorize(str)(things1854concepts)\n",
    "\n",
    "things1854concepts_str[np.char.startswith(things1854concepts_str, 'head')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strokes = ast.literal_eval(things_draw_prod_df[things_draw_prod_df['_id']=='641a2ef188da294e33e766ae'].strokes.values[0])\n",
    "test_undos = ast.literal_eval(things_draw_prod_df[things_draw_prod_df['_id']=='641a2ef188da294e33e766ae'].undo_history.values[0])\n",
    "concept2category_dict['tiara']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "cm = sns.color_palette('viridis')\n",
    "\n",
    "for i, stroke in enumerate(test_strokes):\n",
    "\n",
    "    xs = [x['x'] for x in stroke if x['action']!='end']\n",
    "    ys = [x['y'] for x in stroke if x['action']!='end']\n",
    "    plt.scatter(np.array(xs),550-np.array(ys), color=cm[i])\n",
    "plt.xlim(0,550)\n",
    "plt.ylim(0,550)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# plt.title(f'pen trajectory for {sketch_df.concept[3]}'  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_grid(df, grid_size=8):\n",
    "    # Sample rows from the dataframe\n",
    "    sampled_rows = df.sample(n=grid_size**2, random_state=52)\n",
    "    \n",
    "    # Create a figure with a grid of subplots\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(20, 20))\n",
    "    cm = sns.color_palette('magma_r', n_colors=100)\n",
    "    \n",
    "    for idx, (ax, (_, row)) in enumerate(zip(axes.flatten(), sampled_rows.iterrows())):\n",
    "        # Extract strokes and undo history\n",
    "        strokes = ast.literal_eval(row['strokes'])\n",
    "        undos = ast.literal_eval(row['undo_history'])\n",
    "        \n",
    "        # Plot each stroke\n",
    "        for i, stroke in enumerate(strokes):\n",
    "            xs = [x['x'] for x in stroke if x['action'] != 'end']\n",
    "            ys = [x['y'] for x in stroke if x['action'] != 'end']\n",
    "            ax.scatter(np.array(xs), 550 - np.array(ys), color=cm[i +5])\n",
    "        \n",
    "        # Set the title to the concept\n",
    "        ax.set_title(row['concept'].replace('_',' '), fontsize=20)\n",
    "        \n",
    "        ax.set_xlim(0, 550)\n",
    "        ax.set_ylim(0, 550)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_facecolor('white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sketch_grid_strokes.pdf',dpi=300,bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "render_grid(sketch_trials_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do a pca on the spose embeddings using the spose_cols to subset spose_embeds and visualize the concepts in 3d\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(spose_embeds[spose_cols].values)\n",
    "spose_embeds['pca-one'] = pca_result[:,0]\n",
    "spose_embeds['pca-two'] = pca_result[:,1]\n",
    "spose_embeds['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spose_embeds['category'] = spose_embeds['concept'].apply(lambda x: things_plus_dict[x] if x in things_plus_dict.keys() else 'other')\n",
    "\n",
    "sns.scatterplot(x='pca-one',y='pca-two',data = spose_embeds[spose_embeds['category']=='drink'],hue = 'concept',legend=True)\n",
    "###place the legend outside the plot\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spose_embeds['category'] = spose_embeds['concept'].apply(lambda x: things_plus_dict[x] if x in things_plus_dict.keys() else 'other')\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='pca-one',y='pca-two',data = spose_embeds,hue = 'category',legend=False)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "plt.savefig('VSS2023_pca.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### survey plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "survey_df =  things_draw_prod_df[things_draw_prod_df.trial_type=='survey-text'].reset_index(drop=True)\n",
    "survey_df['response'] = survey_df.apply(lambda x: ast.literal_eval(x.response),axis=1)\n",
    "survey_df['state'] =survey_df['response'].apply(lambda x: x['State'].lower().strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demog_responses = things_draw_prod_df[things_draw_prod_df.trial_type=='survey'].reset_index()\n",
    "demog_responses['response'] = demog_responses['response'].apply(ast.literal_eval)\n",
    "for key in demog_responses.response[0].keys():\n",
    "    demog_responses[key] = demog_responses.response.apply(lambda x: x[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demog_responses.participantEthnicity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "    'alabama': 'AL', 'al': 'AL', 'ala': 'AL', \n",
    "    'alas': 'AK', 'alaska': 'AK', 'ak': 'AK',\n",
    "    'arizona': 'AZ', 'az': 'AZ', 'phoenix, az': 'AZ',\n",
    "    'arkansas': 'AR', 'arkansans': 'AR', 'ar': 'AR',\n",
    "    'california': 'CA', 'calif': 'CA', 'ca': 'CA', 'california ': 'CA', 'san diego county, ca':'CA','la/new orleans':'CA',\n",
    "    'colorado': 'CO', 'colo': 'CO', 'co': 'CO','colorad': 'CO','colorado, usa, 80013': 'CO','colorado, westminister': 'CO','denver': 'CO',\n",
    "    'connecticut': 'CT', 'conn': 'CT', 'ct': 'CT',\n",
    "    'delaware': 'DE', 'del': 'DE', 'de': 'DE', \n",
    "    'district of columbia': 'DC', 'dc': 'DC',\n",
    "    'florida': 'FL', 'fla': 'FL', 'fl': 'FL','fl kissimmee': 'FL',\n",
    "    'georgia': 'GA', 'ga': 'GA','georgia ': 'GA','gerogia': 'GA','georgia, usa': 'GA','atlanta, georgia': 'GA','atlanta':'GA','atlanta ga':'GA',\n",
    "    'hawaii': 'HI', 'hi': 'HI', \n",
    "    'idaho': 'ID', 'id': 'ID', \n",
    "    'illinois': 'IL', 'ill': 'IL', 'il': 'IL','illionois': 'IL','illinois ': 'IL',\n",
    "    'indiana': 'IN', 'ind': 'IN', 'in': 'IN', \n",
    "    'iowa': 'IA', 'ia': 'IA',\n",
    "    'kansas': 'KS', 'kan': 'KS', 'ks': 'KS', \n",
    "    'kentucky': 'KY', 'ky': 'KY', 'ky.': 'KY','kentucky ': 'KY',\n",
    "    'louisiana': 'LA', 'la': 'LA',\n",
    "    'maine': 'ME', 'me': 'ME', \n",
    "    'maryland': 'MD', 'md': 'MD', \n",
    "    'massachusetts': 'MA', 'mass': 'MA', 'ma': 'MA',\n",
    "    'michigan': 'MI', 'mich': 'MI', 'mi': 'MI', 'michihan': 'MI', 'michigan ': 'MI',\n",
    "    'minnesota': 'MN', 'minn': 'MN', 'mn': 'MN',\n",
    "    'mississippi': 'MS', 'miss': 'MS', 'ms': 'MS', \n",
    "    'missouri': 'MO', 'mo': 'MO', 'missouri ': 'MO',\n",
    "    'montana': 'MT', 'mt': 'MT',\n",
    "    'nebraska': 'NE', 'nebr': 'NE', 'ne': 'NE',\n",
    "    'nevada': 'NV', 'nev': 'NV', 'nv': 'NV',\n",
    "    'new hampshire': 'NH', 'nh': 'NH', \n",
    "    'new jersey': 'NJ', 'nj': 'NJ', \n",
    "    'new mexico': 'NM', 'nm': 'NM',\n",
    "    'new york': 'NY', 'ny': 'NY', 'nyc': 'NY',\n",
    "    'north carolina': 'NC', 'n carolina': 'NC', 'nc': 'NC', 'wake forest, nc': 'NC',\n",
    "    'north dakota': 'ND', 'n dakota': 'ND', 'nd': 'ND',\n",
    "    'ohio': 'OH', 'oh': 'OH', 'columbus, ohio': 'OH',\n",
    "    'oklahoma': 'OK', 'okla': 'OK', 'ok': 'OK','oklahoma ':'OK',\n",
    "    'oregon': 'OR', 'ore': 'OR', 'or': 'OR', \n",
    "    'pennsylvania': 'PA', 'pa': 'PA', 'penn': 'PA', 'pennsylvania ': 'PA',\n",
    "    'rhode island': 'RI', 'ri': 'RI',\n",
    "    'south carolina': 'SC', 's carolina': 'SC', 'sc': 'SC',\n",
    "    'south dakota': 'SD', 's dakota': 'SD', 'sd': 'SD', \n",
    "    'tennessee': 'TN', 'tenn': 'TN', 'tn': 'TN',\n",
    "    'texas': 'TX', 'tex': 'TX', 'tx': 'TX', \n",
    "    'utah': 'UT', 'ut': 'UT',\n",
    "    'vermont': 'VT', 'vt': 'VT',\n",
    "    'virginia': 'VA', 'va': 'VA','virginia beach': 'VA','virginia ': 'VA','virginias': 'VA',\n",
    "    'washington': 'WA', 'wash': 'WA', 'wa': 'WA','washington state': 'WA',\n",
    "    'west virginia': 'WV', 'wv': 'WV', \n",
    "    'wisconsin': 'WI', 'wis': 'WI', 'wi': 'WI',\n",
    "    'wyoming': 'WY', 'wyo': 'WY', 'wy': 'WY',\n",
    "    '':'','1':'','north':'','united state':'',\n",
    "}\n",
    "\n",
    "# make all the the text in survey_df.state lowercase\n",
    "survey_df['state_short'] = survey_df['state'].apply(lambda x:state_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "state_counts = survey_df['state_short'].value_counts()\n",
    "state_counts_dict = dict(zip(state_counts.index, state_counts.values))\n",
    "\n",
    "### sor the keys in state_counts_dict alphabetically\n",
    "state_counts_dict = {k: v for k, v in sorted(state_counts_dict.items(), key=lambda item: item[0])}\n",
    "state_counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import json\n",
    "\n",
    "# Create a Choropleth plot\n",
    "choropleth = go.Choropleth(\n",
    "    locations=list(state_counts_dict.keys()),\n",
    "    locationmode='USA-states',\n",
    "    z=list(state_counts_dict.values()),\n",
    "    colorscale='magma_r',\n",
    "    colorbar_title='Count'\n",
    ")\n",
    "\n",
    "# Create a Scattergeo trace\n",
    "scattergeo = go.Scattergeo(\n",
    "    mode='markers',\n",
    "    locationmode='USA-states',\n",
    "    locations=list(state_counts_dict.keys()),\n",
    "    marker=dict(\n",
    "        size=list(state_counts_dict.values()),\n",
    "        sizemode='diameter',\n",
    "        sizeref=2,\n",
    "        color='rgba(255, 0, 0, 0.7)',\n",
    "        line=dict(\n",
    "            width=1,\n",
    "            color='rgba(0, 0, 0, 0.5)'\n",
    "        ),\n",
    "        symbol='circle'\n",
    "    ),\n",
    "    hoverinfo='text',\n",
    "    text=[f\"{state}: {count}\" for state, count in state_counts_dict.items()]\n",
    ")\n",
    "\n",
    "# Create a Figure and add the Choropleth and Scattergeo traces\n",
    "fig = go.Figure(data=[choropleth])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig.update_layout(\n",
    "    geo=dict(\n",
    "        scope='usa',\n",
    "        projection=go.layout.geo.Projection(type='albers usa'),\n",
    "        center=dict(lon=-95, lat=37),\n",
    "    \n",
    "    ),\n",
    "        height=500,\n",
    "        width=800,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_draw_prod_df[things_draw_prod_df.trial_type=='survey'].response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### only include rows in things_draw_recog_demo_df if the workerID is in things_draw_recog_df.workerID\n",
    "\n",
    "things_draw_recog_demo_df = things_draw_recog_demo_df[things_draw_recog_demo_df.workerID.isin(things_draw_recog_df.workerID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in things_draw_recog_demo_df if there are multiple rows with the same workerID, keep the row that appears first\n",
    "things_draw_recog_demo_df = things_draw_recog_demo_df.drop_duplicates(subset=['workerID'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_draw_recog_demo_df.response.apply(lambda x: eval(x)['participant_sex'].lower().strip()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_draw_recog_demo_df.response.apply(lambda x: int(eval(x)['participant_age'])).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_draw_recog_df.workerID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages =[]\n",
    "for this_age in range(things_draw_prod_df.workerID.nunique()):\n",
    "    ages.append(things_draw_recog_demo_df.sample(1).response.apply(lambda x: int(eval(x)['participant_age'])))\n",
    "np.mean(ages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prod df statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_demographic_trials = things_draw_prod_df[things_draw_prod_df.trial_type=='survey'].reset_index(drop=True)\n",
    "prod_demographic_trials = prod_demographic_trials.drop_duplicates(subset=['workerID'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_demographic_trials.response.apply(lambda x: eval(x)['participantSex'].lower().strip()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1314 - 735 - 511 -18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "things-drawing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
